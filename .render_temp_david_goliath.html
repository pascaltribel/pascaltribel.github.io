<html>
<head>
	<link rel="stylesheet" href="bootstrap.css">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
	<!-- HTML Meta Tags -->
	<title>Why make it complicated when you can make it simple ?</title>
	<meta name="description" content="hy make it complicated when you can make it simple?">
	<meta property="og:description" content="Science is overly fed with complex Machine Learning models. I think this is often not a good thing.">
	
	<!-- Facebook Meta Tags -->
	<meta property="og:url" content="https://pascaltribel.github.io/david_goliath.html">
	<meta property="og:type" content="Blog article">
	<meta property="og:title" content="Why make it complicated when you can make it simple?">
	<meta property="og:description" content="Science is overly fed with complex Machine Learning models. I think this is often not a good thing.">
	<meta property="og:image" content="david_goliath.jpg">
	
</head>
<body>



<div class="container-fluid p-0">
	 	<nav class="navbar navbar-expand-sm navbar-dark bg-dark">
			<a class="navbar-brand" href="index.html">Pascal Tribel</a>
			<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
					<span class="navbar-toggler-icon"></span>
			</button>
			<div class="collapse navbar-collapse" id="navbarSupportedContent">
				<ul class="navbar-nav mr-auto">
					<li class="nav-item active">
						<a class="nav-link" href="index.html">Home <span class="sr-only">(current)</span></a>
					</li>
					<li class="nav-item">
						<a class="nav-link" href="cv.html">CV</a>
					</li>
					<li class="nav-item">
						<a class="nav-link" href="https://github.com/pascaltribel/">Github</a>
					</li>
				</ul>
			</div>
		</nav>
</div>

<div class="container-fluid p-3">	
	<div class="row justify-content-center">
		<div class="col-lg-6">
			<hr class="my-4">
			<h2>Why make it complicated when you can make it simple ?</h2>
			<i>September 2024</i>
			<hr class="my-4">
		<p class="text-justify">
			In recent years, the rise of complex and non-linear approaches, particularly those powered by <i>Deep Learning</i> architectures, has become an extremely present trend across a wide range of disciplines. Initially gaining prominence in computer science, these deep, convolutional, recurrent, or residual neural networks, and other transformer models have rapidly extended their reach into fields as diverse as physics, chemistry, biology, geography, geology, mathematics and many others. In sociology and, notably, more recently in linguistics, these models have started to shape the way researchers approach problems, proposing new tools to tackle old challenges. However, the allure of these sophisticated methods often overshadows a critical understanding of their <b>limitations</b> and the <b>assumptions embedded within them</b>. While these techniques often offer reasonable predictive capabilities, their widespread adoption is often done without questioning their suitability and the potential risks of over-reliance on such intricate systems in areas where simpler methods might suffice or even be preferable.
		</p>
		<p class="text-justify">
			Despite the remarkable success of these models, they often carry with them a set of strong, but <b>often implicit</b>, assumptions that are rarely scrutinized. <i>Non-linearity</i>, for instance, is frequently assumed to be an inherent quality of the problems being addressed, without sufficient consideration of whether a simpler, linear approach might be equally effective. The sheer number of parameters and variables that these models require is another significant assumption. The most famous consequence this assumption can lead to is overfitting, where the model becomes too tailored to the specific data it was trained on, rather than generalizing well to new, unseen data. Of course, this is a practical problem, but this assumption also comes with the idea that <i>a</i> (really huge and) <i>complex model is indeed a good representation of the reality underlying the observed data</i>. These assumptions are often accepted uncritically in the Machine Learning process, with the focus being on <i>optimizing performance</i> rather than <b>questioning the fundamental premises on which these models are built</b>.
		</p>
		<p class="text-justify">
			These huge non-linear models are now more than ever employed <b>as if they were the best or even the only approach</b>, regardless of the nature of the problem being studied. As an example, the hypothesis of non-linearity, even locally, is highly discussable: many problems may, in fact, be linear or locally linear, and this would allow the use of all the powerful <i>linear artillery</i>. Indeed, the vast array of linear models and other simpler methods, which have been <i>developed and refined over decades</i> and are known for their <b>robustness</b> and <b>interpretability</b>, are often overlooked. Tools such as linear regression, despite their simplicity, can offer powerful insights when applied correctly and can be much more transparent and easier to interpret than their complex counterparts. The failure to compare complex neural networks with these simpler, well-understood models means that researchers may be missing out on more straightforward solutions that could not only provide a <b>clearer understanding</b> of the underlying processes at play, but also, sometimes, yield better results. This <i><b>over-reliance on complexity</b></i>, without sufficient justification, raises concerns about the direction in which the field is heading and whether it is losing sight of the importance of simplicity and clarity in scientific inquiry.
		</p>
		<p class="text-justify">
			The widespread adoption of complex models is often driven by a <b>fallacious line of reasoning</b>: the assumption that <i>because these models perform well on complex problems, they must also be effective for simpler ones, and, furthermore, that if these models work well on simple problems, they must somehow be more closely aligned with the underlying reality of the phenomena being studied</i>.
		</p>
		<p class="text-justify">
			This reasoning is <i>flawed</i>, of course, and misleading, as it can lead researchers to overestimate the capabilities of these models and to overlook simpler, more transparent approaches that might be more appropriate. The belief that <b>complexity necessarily equates to accuracy or realism is in contradiction with the idea that the simplest explanation is often the best</b>, as suggested by Ockham's razor. Furthermore, the focus on achieving high performance metrics, such as <i>accuracy</i> or <i>precision</i>, can come at the expense of other important factors, such as <i>model interpretability</i>, <i>robustness</i>, and <i>generalizability</i>. In many cases, the use of complex models is justified primarily by their ability to achieve high performance on benchmark datasets, rather than by their ability to provide meaningful insights into the underlying phenomena being studied.
		</p>
		<p class="text-justify">
			As mentioned earlier, the success of complex methods on problems that appear complex at first glance does not necessarily mean that these models are the <i>most appropriate</i>. In many cases, the complexity of the problem may be overstated, and simpler models may be capable of achieving comparable or even superior results. Moreover, the recent emphasis on performance and precision over explainability and model transparency reflects a <b>broader cultural shift</b> that <b>prioritizes outcomes over understanding</b>. This shift is, of course, socio-cultural, but in the field of computer sciences, it has been enabled by the rapid increase in computational power, which allows for the training and deployment of increasingly complex models. However, this focus on performance can lead to a neglect of the underlying principles that should guide scientific inquiry, such as the importance of <i>transparency</i>, <i>interpretability</i>, and the <i>ability to explain the results in a meaningful way</i>. The use of models that cannot be, to a great extent, explained or understood is particularly problematic in the context of scientific research, where the goal is, to me, not only to make accurate predictions but also and especially to <b>gain a deeper understanding of the underlying processes</b>. While this <i>results-oriented</i> approach may be necessary in certain applied contexts, such as engineering, it is, to my sense, less appropriate in the theoretical sciences, where the goal is to advance knowledge and understanding rather than simply to achieve practical results.
		</p>
		<p class="text-justify">
			Finally, and perhaps most importantly, the widespread adoption of complex methods leads to a concerning oversight: the <b>confusion between models and reality</b>. It is essential to remember that models are, at their core, simplifications of the observations of reality, designed to help us understand and predict certain aspects of the world around us. However, <i>they are not reality itself</i>. The rational, mathematical, and logical approaches that underpin these models allow us to capture and describe measured phenomena, but they do not claim to represent the full complexity of reality. For centuries, scientists have developed models with a <i>deep awareness of the philosophical questions surrounding the notion of modeling reality</i>. These questions, which touch on issues such as the <b>limits of knowledge</b>, the <b>nature of truth</b>, and the <b>relationship between the observer and the observed</b>, were considered central to the scientific enterprise. However, in the current era of Machine Learning, these philosophical concerns are often neglected, forgotten, or <i>outright dismissed</i>. Despite the poorness of the reflexion that follows this neglection, this is particularly troubling given the increasing reliance on models in decision-making processes across a wide range of fields, from finance, to healthcare, and most importantly to public policy and politics. 
		</p>
		<p class="text-justify">
			All of this is regrettable. 
		</p>
		<p class="text-justify">
			Of course, some problems are <b>likely</b> only to be tackled effectively with complex models due to the <b>intricate nature of the data and phenomena involved</b>. 
However, it is crucial that such models are used with <b>parsimony</b> rather than out of <b>scientific laziness</b>.
		</p>
		<p class="text-justify">
			The <a href="https://www.erudit.org/fr/revues/npss/2012-v7-n2-npss0355/">"New Perspectives in the Social Sciences"</a> journal offers a valuable opportunity for reflection on these issues. Of course, the opinions expressed in these articles are those of their respective authors and do not necessarily reflect my own views.
		</p>

		</div>
	</div>
</div>

</body>
</html>
<script>window.scrollTo(0, 1304); setTimeout(function() { window.scrollTo(0, 1304); },200)</script>